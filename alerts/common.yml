groups:

  - name: CommonAlerts
    rules:


#      - alert: SSLCertExpiresSoon
#        expr: probe_ssl_earliest_cert_expiry { job = "blackbox_certs_expiration" } - time() < (60 * 60 * 24 * 14)
#        for: 10m
#        labels:
#          severity: "{{ $labels.severity }}"
#        annotations:
#          summary: "SSL certificate for {{ $labels.instance }} expires soon!"

      - alert: DiskFreeLessThan15%
        expr: ( 100 - ( node_filesystem_avail_bytes { mountpoint !~ "/boot|/home", instance!="anyway.gateway.prod.1" } / node_filesystem_size_bytes { mountpoint !~ "/boot|/home", instance!="anyway.gateway.prod.1" } * 100 ) ) > 85
        for: 1m
        labels:
          severity: medium
          owner: "{{ $labels.owner }}"
        annotations:
          summary: 'There is {{ $value | printf "%.2f"}}% of disk space used! Threshold 85%'
          advice:  'Cleanup disk. Instructions: https://kb.bank.rrr.vip-clients/pages/viewpage.action?pageId=451335341'
          help: 'df -h'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}


      - alert: TargetOrExporterProdDown
        expr: up { job !~ "pods(-extra.*)?", type!="patroni"} != 1
        for: 1m # [FIXME] we better have a lower value (of, say, 30s) here and solve false alerts sent on pod startup somehow else (check pod/deployment/replicationController creation time? check (pod|container)'s start time?)
        labels:
          #severity: high
          severity: "{{ $labels.severity }}"
        annotations:
          summary: "No probe success! Target (or exporter) is down in {{ $labels.job }}!"
          advice: "Figure out which one is down and alert necessary people!"
          help: "up{job!~'{{ $labels.job }}'} != 1"
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: BlackboxProbeProdDown
        expr: probe_success { project = "", env = "prod" } != 1
        for: 30s
        labels:
          severity: "{{ $labels.severity }}"
          owner: common
        annotations:
          summary: "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 30s"
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: AppAllPodsDownForService
        expr: sum ( up { job = "pods", env = "prod" } ) by ( namespace, app ) == 0
        for: 30s
        labels:
          severity: high
          pod: 
        annotations:
          summary: '0 pods are available in a service (or all pods have unreachable metrics endpoints)! Downtime > 30s.'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: PostgresInactiveReplSlots
        expr: pg_repl_inactive_slots { env = "prod" } > 0
        for: 3m
        labels:
          severity: high
        annotations:
          summary: 'Inactive replication slot(s) in PostgreSQL! Inactive > 3m'
          advice: 'Check Debezium pods. Check cluster replication.'
          help: 'SELECT * FROM pg_replication_slots WHERE active = false;'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: KafkaConsumerGroupLagInTopics
        expr: sum ( kafka_consumergroup_lag { env = "prod" } > 100 ) by ( consumergroup, topic )
        for: 5m
        labels:
          severity: medium
        annotations:
          summary: 'Consumer group started to lag while reading some Kafka topic. Duration > 10m, lag size > 10.'
          advice: 'Check the state of microservice, related to the consumer. Watch the lag: is it only growing? If yes - then it might be an abandoned subscription or the situation where we produce messages faster than we can consume, if no - might be a short burst of new messages, just wait until the lag drops to 0.'
          help: "Go to 'General > Kafka Exporter Overview' dashboard in Grafana to find the lagging (consumergroup, topic)."

      - alert: KafkaBrokersDown
        expr: kafka_brokers { env = "prod" } < 2
        for: 30s
        labels:
          severity: high
        annotations:
          summary: 'Some Kafka brokers are down! Downtime > 30s.'
          advice: "Alert owners of Kafka (see 'Contacts' article in Confluence) ASAP!"
          help: "Kafka Tool may be used to confirm that some brokers are down."
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: BitbucketDown
        expr: probe_success { job = "blackbox_exporter", instance = "bitbucket" } != 1
        for: 30s
        labels:
          severity: medium
        annotations:
          summary: "Bitbucket server is down! Down > 30s"
          advice: "Alert CI/CD team if this is not a planned downtime!"
          help: "https://bitbucket.domain/ is expected to work."
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: GibRabbitMqDown
        expr: probe_success { job = "blackbox_exporter", instance = "gib.rabbitmq" } != 1
        for: 30s
        labels:
          severity: high
          owner: groupib
        annotations:
          summary: "GroupIB's RabbitMQ:5672 is down! Down > 30s"
          advice: "Alert GroupIB!"
          help: "It affects securebank-atomic."
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: QuayDown
        expr: probe_success { job = "blackbox_exporter", instance = "quay" } != 1
        for: 30s
        labels:
          severity: high
        annotations:
          summary: "Quay HTTP-probe didn't return success!"
          advice: "Alert CI/CD team if this is not a planned downtime!"
          help: "http://quay.paas.bank.rrr.vip-clients/ is expected to work."
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: QulixIntegrationDown
        expr: probe_success { job = "blackbox_exporter", instance = "qulix.integration" } != 1
        for: 30s
        labels:
          severity: high
        annotations:
          summary: "Qulix.Integration HTTP-probe didn't return success!"
          advice: "Check if Qulix.Integration is working fine, if not - alert qulixoids."
          help: 'curl "http://192.168.1.2:9099/integration/version.txt"'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

