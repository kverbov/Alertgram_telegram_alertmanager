groups:

###################################################################
# DBO EVO
###################################################################

#  - name: AntiFraudSystems
#    rules:
#
#      - alert: AFSKafkaConsumerGroupLagInTopics
#        expr: sum ( kafka_consumergroup_lag > 100 )

  - name: EvoAlerts
    rules:

      - alert: EvoBlackboxProbeDown
        expr: probe_success { project = "evo", env = "prod" } != 1
        for: 30s
        labels:
          severity: normal
          #severity: '{{ $labels.severity }}'
          owner: profiteroles
        annotations:
          summary: "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 30s"
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

#      - alert: EvoWTF
#        expr: pg_up { project = 'evo', env = 'prod', job = 'postgresql_exporter' } > 0
#        for: 30s
#        labels:
#          severity: high
#          env: prod
#          owner: profiteroles
#        annotations:
#          summary: 'Ignore it. John Smith. wtf3'

      - alert: EvoGraylogUncommittedMessagesLag
        expr: graylog_uncommitted_count { project = "evo", env =~ 'cert|prod' } > 50000
        for: 30s
        annotations:
          summary: "There are more than 50k uncommitted messages in [{{ $labels.env }}] Graylog. Instructions: https://kb.bank.rrr.vip-clients/display/degaulle/Troubleshooting"
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: EvoGraylogUncommittedMessagesLagTest
        expr: graylog_uncommitted_count { project = "evo", env = "test" } > 3000000
        for: 30s
        annotations:
          summary: "There are more than 3M uncommitted messages in [{{ $labels.env }}] Graylog. Instructions: https://kb.bank.rrr.vip-clients/display/degaulle/Troubleshooting"
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: KafkaTopicReplicationLag
        expr: avg(consumergroup_topic:kafka_consumergroup_lag:sum { env = "prod", project = "evo" , topic=~"rdbo.prod.operation.*|rdbo.prod.transaction.*|rdbo.prod.cdc.clnt.crossreference|rdbo.prod.accountbridge.task.execute.v0|rdbo.prod.accountbridge.accounts.snapshot.v0|rdbo.prod.cdc.uprf.sessionevent|rdbo.prod.cardholder.task.execute.v2|rdbo.prod.cardholder.card.update.full.v1", consumergroup=~"evo_operation.*|operation.*|transaction|userprofile-client-group-prod|evo_accountbridge_task|evo_accounts_bridge_v4|evo_card_session_v3|evo_cardholder_task_v2|evo_card_cardupdate" }) by (consumergroup, topic, owner) > 3000
        for: 30s
        labels:
            severity: warning
            project: "evo"
            env: "prod"
            owner: '{{ $labels.owner }}'
        annotations:
          description: 'Important kafka topic have high replication lag {{ $value | printf "%.2f"}}. Check consumer!'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: EvoPostgresDown
        expr: pg_up { project = "evo", env = "prod" } != 1
        for: 30s
        labels:
          severity: high
          owner: profiteroles
        annotations:
          summary: "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 30s"
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: EvoPostgresWalTooBig
        expr: pg_wals_size { project = "evo" } > 16106127360 # 15 * 1024^3
        for: 3m
        labels:
          owner: profiteroles
          severity: high
        annotations:
          summary: 'WAL is too big {{ humanize $value }} on {{ $labels.instance }} PostgreSQL! > 3m.  Threshold 15Gb.'
          advice: 'https://kb.bank.rrr.vip-clients/pages/viewpage.action?pageId=451344879'
          help: 'ALTER SYSTEM SET archive_command=":"; SELECT pg_reload_conf(); as a temp solution.'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: EvoPostgresInactiveReplSlots
        expr: pg_repl_slot_active_state { project = "evo" } == 0
        for: 3m
        labels:
          owner: profiteroles
          severity: high
        annotations:
          summary: 'Inactive replication slot(s) in {{ $labels.instance }} PostgreSQL! Inactive > 3m'
          advice: 'Check Debezium pods. Check cluster replication. https://kb.bank.rrr.vip-clients/pages/viewpage.action?pageId=451344879'
          help: 'SELECT * FROM pg_replication_slots WHERE active = false;'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: EvoDbzmPodsRestartingTooFrequently
        expr: sum((rate ( kube_pod_container_status_restarts_total { project = "evo", namespace =~ 'dbzm.+', env =~ 'cert|prod', instance !~ "debezium-gmfn.+" } [10m] ) * 600) * on(uid) group_right(pod) kube_pod_info) by (deployconfig,namespace,node,env,pod,owner,project) >= 2
        #expr: kube_pod_info * on(uid) group_left(pod) (rate ( kube_pod_container_status_restarts_total { project = "evo", namespace =~ 'dbzm.+', env =~ 'cert|prod' } [10m] ) * 600) >= 2
        for: 30s
        labels:
          owner: profiteroles
          severity: high
        annotations:
          #summary: 'Pods are restarting too frequently (during past 10 min). Problem with a few pods: ask around (did anyone deploy anything new?) and alert the team responsible for the service; Problem with lots of pods (different services) - alert OpenShift cluster admins and EVO devops ASAP!'
          summary: 'Pods are restarting too frequently (during past 10 min). alert OpenShift cluster admins and EVO devops ASAP!'
          #advice: 'Most of the time the problem is just with a few pods and the most usual reason is service misconfiguration. But sometimes due to failures of liveness/readiness probes (services check availability of the systems they depend on).'
          help: "Go to 'Health > Health (rdbo|ipsm)' dashboard to find the restarting pods. Visit linker > Microservices to find the responsible dev team."
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: EvoDbzmPodsRestartingTooFrequently_gmfn
        expr: sum((rate ( kube_pod_container_status_restarts_total { project = "evo", namespace =~ 'dbzm.+', env =~ 'cert|prod', instance =~ "debezium-gmfn.+" } [10m] ) * 600) * on(uid) group_right(pod) kube_pod_info) by (deployconfig,namespace,node,env,pod,owner,project) >= 2
        for: 30s
        labels:
          owner: profiteroles
          severity: high
        annotations:
          summary: 'Pods are restarting too frequently (during past 10 min). alert OpenShift cluster admins and EVO devops ASAP!'
          help: "Go to 'Health > Health (rdbo|ipsm)' dashboard to find the restarting pods. Visit linker > Microservices to find the responsible dev team."
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}
            
      - alert: EvoPodToDependentResourceFail
        #expr: anyWay_up { project = "evo", env = "prod", job = "pods-extra" } != 1
        expr: >-
            count (
                label_replace(
                    { project = "evo", env = "prod", job = "pods-extra", __name__ =~ '.+_up' },
                    "dependent_resource",
                    "$1",
                    "__name__",
                    "(.+)_up"
                ) != 1
            )
            by (origin_prometheus, project, env, namespace, dc, owner, dependent_resource) > 0
        for: 30s
        labels:
          severity: high
        annotations:
          #summary: "Pod {{ $labels.instance }} reports: Anyway is down for more than 30s"
          summary: "{{ $value }} pod(s) in [{{ $labels.origin_prometheus }}/{{ $labels.namespace }}] of {{ $labels.dc }} report(s): {{ $labels.dependent_resource }} is down for more than 30s"
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

#      - alert: EvoPodToAnywayFail
#        #expr: anyWay_up { project = "evo", env = "prod", job = "pods-extra" } != 1
#        expr: count (anyWay_up { project = "evo", env = "prod", job = "pods-extra" } != 1) by (origin_prometheus, project, env, namespace, dc, owner) > 0
#        for: 30s
#        labels:
#          severity: high
#        annotations:
#          #summary: "Pod {{ $labels.instance }} reports: Anyway is down for more than 30s"
#          summary: "{{ $value }} pod(s) in [{{ $labels.origin_prometheus }}/{{ $labels.namespace }}] of {{ $labels.dc }} report(s): Anyway is down for more than 30s"
#
#      - alert: EvoPodToAtmFail
#        #expr: atm_up { project = "evo", env = "prod", job = "pods-extra" } != 1
#        expr: count (atm_up { project = "evo", env = "prod", job = "pods-extra" } != 1) by (origin_prometheus, project, env, namespace, dc, owner) > 0
#        for: 30s
#        labels:
#          severity: high
#        annotations:
#          #summary: "Pod {{ $labels.instance }} reports: ATM is down for more than 30s"
#          summary: "{{ $value }} pod(s) in [{{ $labels.origin_prometheus }}/{{ $labels.namespace }}] of {{ $labels.dc }} report(s): ATM is down for more than 30s"
#
#      - alert: EvoPodToHumanFactorLabFail
#        #expr: hlf_up { project = "evo", env = "prod", job = "pods-extra" } != 1
#        expr: count (hlf_up { project = "evo", env = "prod", job = "pods-extra" } != 1) by (origin_prometheus, project, env, namespace, dc, owner) > 0
#        for: 30s
#        labels:
#          severity: high
#        annotations:
#          #summary: "Pod {{ $labels.instance }} reports: Human Factor Labs is down for more than 30s"
#          summary: "{{ $value }} pod(s) in [{{ $labels.origin_prometheus }}/{{ $labels.namespace }}] of {{ $labels.dc }} report(s): Human Factor Labs is down for more than 30s"
#
#      - alert: EvoPodToOfficeFail
#        #expr: office_up { project = "evo", env = "prod", job = "pods-extra" } != 1
#        expr: count (office_up { project = "evo", env = "prod", job = "pods-extra" } != 1) by (origin_prometheus, project, env, namespace, dc, owner) > 0
#        for: 30s
#        labels:
#          severity: high
#        annotations:
#          #summary: "Pod {{ $labels.instance }} reports: Office is down for more than 30s"
#          summary: "{{ $value }} pod(s) in [{{ $labels.origin_prometheus }}/{{ $labels.namespace }}] of {{ $labels.dc }} report(s): Office is down for more than 30s"
#
#      - alert: EvoPodToWebbybankDomFail
#        #expr: Rrr-bla-bla { job = "pods-extra", env = "prod" } != 1
#        expr: count (Rrr-bla-bla { project = "evo", env = "prod", job = "pods-extra" } != 1) by (origin_prometheus, project, env, namespace, dc, owner) > 0
#        for: 30s
#        labels:
#          severity: high
#        annotations:
#          summary: "{{ $value }} pod(s) in [{{ $labels.origin_prometheus }}/{{ $labels.namespace }}] of {{ $labels.dc }} report(s): Webby.bankDom is down for more than 30s"
#
#      - alert: EvoPodTorrrfinanceBankFail
#        #expr: rfb_up { project = "evo", env = "prod", job = "pods-extra" } != 1
#        expr: count (rfb_up { project = "evo", env = "prod", job = "pods-extra" } != 1) by (origin_prometheus, project, env, namespace, dc, owner) > 0
#        for: 30s
#        labels:
#          severity: high
#        annotations:
#          #summary: "Pod {{ $labels.instance }} reports: rrrfinance Bank is down for more than 30s"
#          summary: "{{ $value }} pod(s) in [{{ $labels.origin_prometheus }}/{{ $labels.namespace }}] of {{ $labels.dc }} report(s): rrrfinance Bank is down for more than 30s"
#
#      - alert: EvoPodToTwpgFail
#        #expr: twpg_up { project = "evo", env = "prod", job = "pods-extra" } != 1
#        expr: count (twpg_up { project = "evo", env = "prod", job = "pods-extra" } != 1) by (origin_prometheus, project, env, namespace, dc, owner) > 0
#        for: 30s
#        labels:
#          severity: high
#        annotations:
#          #summary: "Pod {{ $labels.instance }} reports: TWPG is down for more than 30s"
#          summary: "{{ $value }} pod(s) in [{{ $labels.origin_prometheus }}/{{ $labels.namespace }}] of {{ $labels.dc }} report(s): TWPG is down for more than 30s"

      - alert: EvoPodToRedisFail
        expr: redis { project = "evo", env = "prod", job = "pods-extra" } != 1
        #expr: count (redis { project = "evo", env = "prod", job = "pods-extra" } != 1) by (origin_prometheus, inastance, project, env, namespace, dc, owner) > 0
        for: 30s
        labels:
          severity: high
        annotations:
          #summary: "Pod {{ $labels.instance }} reports: Redis (KeyDB?) is down for more than 30s"
          summary: "{{ $value }} pod(s) in [{{ $labels.origin_prometheus }}/{{ $labels.namespace }}] of {{ $labels.dc }} report(s): Redis (KeyDB?) is down for more than 30s"
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      #- alert: BlackboxProbeDown
      #  expr: probe_success { project = "", env = "prod" } != 1
      #  for: 30s
      #  labels:
      #    owner: profiteroles
      #  annotations:
      #    summary: "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 30s"

      - alert: EvoAppPodsRestartingTooFrequently
        expr: sum((rate ( kube_pod_container_status_restarts_total { project = "evo", env = "prod", namespace != "dbzm-prod" } [10m] ) * 600) * on(uid) group_right(pod) kube_pod_info) by (deployconfig,namespace,node,env,pod,owner,project) >= 2
        #expr: kube_pod_info * on(uid) group_left(pod) (rate ( kube_pod_container_status_restarts_total { project = "evo", env = "prod", namespace != "dbzm-prod" } [10m] ) * 600) >= 2
        for: 30s
        labels:
          severity: normal
          #severity: '{{ $labels.severity }}'
          owner: '{{ $labels.owner }}'
        annotations:
          #summary: 'Pods are restarting too frequently (during past 10 min). Problem with a few pods: ask around (did anyone deploy anything new?) and alert the team responsible for the service; Problem with lots of pods (different services) - alert OpenShift cluster admins and EVO devops ASAP!'
          summary: 'Pods are restarting too frequently (during past 10 min). alert OpenShift cluster admins and EVO devops ASAP!'
          #advice: 'Most of the time the problem is just with a few pods and the most usual reason is service misconfiguration. But sometimes due to failures of liveness/readiness probes (services check availability of the systems they depend on).'
          help: "Go to 'Health > Health (rdbo|ipsm)' dashboard to find the restarting pods. Visit linker > Microservices to find the responsible dev team."
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: Alert_pod_CPU
        expr: sum (rate (container_cpu_usage_seconds_total{project = "evo", env = "prod", image!=""}[1m])) by (deployconfig,namespace,env,pod,owner,project) > 5
        for: 30s
        labels:
          severity: normal
          owner: '{{ $labels.owner }}'
          env: 'prod'
        annotations:
          summary: 'Pod is consuming high CPU {{ $value | printf "%.2f"}} cores avg in 1 m'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: Alert_topic_high_write_rate
        expr: avg(rate(topic:kafka_topic_partition_current_offset:sum {topic !~ "^__.*", topic !~ ".*cdct.*", topic != "heartbeats", env = "prod", project = "evo"} [5m])) by (topic) > 50000
        for: 30s
        labels:
          severity: normal
          owner: '{{ $labels.owner }}'
          env: 'prod'
        annotations:
          summary: 'Kafka topic is flooding kafka by messages! {{ $value | printf "%.2f"}} messages in 5 minutes'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}
            
#      - alert: TargetOrExporterDown
#        expr: up { job !~ "pods(-extra.+)?", env = "prod" } != 1
#        for: 30s
#        labels:
#          #severity: high
#          severity: '{{ $labels.severity }}'
#        annotations:
#          summary: "No probe success! Target (or exporter) is down!"
#          advice: "Figure out which one is down and alert necessary people or fix it yourself!"
#          help: 'up{job!~"pods(-extra.+)",env="prod"}!=1'
###NodeExporter_HOST_UTIL_rules_PROD###
      - alert: EvoHostHighCPULoad
        expr: sum((100 -(avg by(instance) (rate(node_cpu_seconds_total{mode="idle",project="evo",job="node_exporter",env="prod"}[5m])) * 100)) * on (instance) group_right (domainname) node_uname_info) by (instance, env, nodename) > 65
        for: 5m
        labels:
          severity: high
          owner: '{{ $labels.owner }}'
          project: evo
          env: prod
        annotations:
          summary: 'CPU utilization is {{ $value | printf "%.2f" }} % for more than 5m. Threshold 65% CPU util.'
          advice: 'Check CPU utilization. Instructions: https://kb.bank.rrr.vip-clients/pages/viewpage.action?pageId=451335178'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: EvoHostHighCpuIOWait
        expr: sum((avg by(instance) (rate(node_cpu_seconds_total{mode="iowait",project="evo",job="node_exporter",env="prod"}[6m])) * 100) * on (instance) group_right (domainname) node_uname_info) by (instance, env, nodename) > 30
        for: 5m
        labels:
          severity: high
          owner: '{{ $labels.owner }}'
          project: evo
          env: prod
        annotations:
          summary: 'CPU IOWAIT is {{ $value | printf "%.2f" }} % for more than 6m. Threshold 30% IOwaits'
          advice: 'Check disks utilization'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: EvoHostOutOfMemory
        expr: (node_memory_MemAvailable_bytes {job="node_exporter",project="evo",env="prod"} / node_memory_MemTotal_bytes * 100) < 10
        for: 2m
        labels:
          severity: high
          owner: '{{ $labels.owner }}'
          project: evo
          env: prod
        annotations:
          summary: 'Host out of memory. Node memory is filling up ({{ $value | printf "%.2f"}}% left). Threshold 10% free memory.'
          advice: 'Check MEMORY utilization'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: HostMemoryUnderMemoryPressure
        expr: sum(rate(node_vmstat_pgmajfault{job="node_exporter",project="evo",env="prod"}[1m]) * on (instance) group_right (domainname) node_uname_info) by (instance, env, nodename) > 1000
        for: 2m
        labels:
          severity: warning
          owner: '{{ $labels.owner }}'
          project: evo
          env: prod
        annotations:
          summary: 'The node is under heavy memory pressure. High rate of major page faults {{ $value | printf "%.2f"}}. Threshold 1000 pagefaults'
          advice: 'Check MEMORY utilization and find out the page faults causing app. Threshold 1000 pagefaults'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: HostOutOfInodes
        expr: node_filesystem_files_free{job="node_exporter",project="evo",env="prod"} / node_filesystem_files * 100 < 10
        for: 1m
        labels:
          severity: warning
          owner: '{{ $labels.owner }}'
          project: evo
          env: prod
        annotations:
          summary: 'Host out of inodes {{ $value | printf "%.2f"}} % left. Threshold 10%.'
          help: 'Check df -hi on host'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

#      - alert: HostUnusualDiskWriteRate
#        expr: sum by (instance) (rate(node_disk_written_bytes_total{job="node_exporter",project="evo"}[2m])) / 1024 / 1024 > 500
#        for: 5m
#        labels:
#          severity: warning
#          owner: '{{ $labels.owner }}'
#          #owner: 'kopylov'
#          project: evo
#          env: prod
#        annotations:
#          summary: 'Disk is probably writing too much data {{ $value | printf "%.2f"}} MB/s. Threshold 500 MB/s.'

###PostgresExporter_rules###
      - alert: DatabaseActiveConnections
        expr: (sum (pg_stat_database_numbackends{project="evo",env="prod"}) by (instance))/(sum (pg_settings_max_connections{project="evo",env="prod"}) by (instance))*100 > 55
        for: 1m
        labels:
          severity: warning
          owner: '{{ $labels.owner }}'
          #owner: 'kopylov'
          project: evo
          env: prod
          #env: {{ $labels.env }}
        annotations:
          summary: 'Too many database connections {{ $value | printf "%.2f"}} % consumed. Threshold 55%'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: pg_publication_count #ACHTUNG!!! custom class, custom metric
        expr: pg_publication_count{job="postgresql_exporter", project="evo", env="prod"} == 0
        for: 5m
        labels:
          severity: warning
          #owner: 'kopylov'
          owner: '{{ $labels.owner }}'
          project: evo
          env: prod
        annotations:
          summary: "Replication of one of the pg_publication_tables table has failed"
          help: "select * from pg_catalog.pg_publication left join pg_catalog.pg_replication_slots on pg_replication_slots.slot_name=pg_publication.pubname"
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: DiskFreeLessThan5%_PROD
        expr: ( 100 - ( node_filesystem_avail_bytes { mountpoint !~ "/boot|/home", project = "evo", env="prod", instance!="anyway.gateway.prod.1" } / node_filesystem_size_bytes { mountpoint !~ "/boot|/home", project = "evo", env="prod", instance!="anyway.gateway.prod.1" } * 100 ) ) > 95
        for: 1m
        labels:
          severity: high
          owner : profiteroles
          project: evo
        annotations:
          summary: 'There is {{ $value | printf "%.2f"}}% of disk space used!!! Threshold 95%'
          advice: 'Cleanup disk ASAP!!! Instructions: https://kb.bank.rrr.vip-clients/pages/viewpage.action?pageId=451335341'
          help: 'df -h'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: KafkaTopicFromGibNotGettingMessages
        expr: avg(rate(topic:kafka_topic_partition_current_offset:sum { topic = "rdbo.prod.securebank.gib.v1", env = "prod", project = "evo"} [5m])) by (topic) < 1
        for: 1m
        labels:
          severity: warning
          owner : profiteroles
          project: evo
        annotations:
          summary: 'В топике rdbo.prod.securebank.gib.v1 {{ $value | printf "%.2f" }} сообщений за 5 минут. Порог < 1.'
          advice: 'https://kb.bank.rrr.vip-clients/pages/viewpage.action?pageId=451342741'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: PatroniMasterChange
        expr: up { type="patroni"} != 1 
        for: 5m
        labels:
            severity: normal
            owner: profiteroles
            env: prod
        annotations:
          summary: '{{ $labels.instance }} is not master anymore'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: PatroniMasterChangeback
        expr: up { type="patroni"} != 0 
        for: 5m
        labels:
            severity: normal
            owner: profiteroles
            env: prod
        annotations:
          summary: '{{ $labels.instance }} is master!'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: TestDatabaseReplicationLag
        expr: pg_repl_lag > 3221225472 # 1024^3*3
        for: 30s
        labels:
            severity: warning
            owner: 'kopylov'
            env: 'test'
        annotations:
          summary: 'Replication lag is more then {{ humanize $value }} on {{ $labels.instance }} PostgreSQL!'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}
###NodeExporter_HOST_UTIL_rules_DEV|TEST|CERT###
      - alert: EvoHostHighCPULoad
        expr: sum((100 -(avg by(instance) (rate(node_cpu_seconds_total{mode="idle",project="evo",job="node_exporter",instance!="ja_android",env=~"test|devtest|dev|cert"}[5m])) * 100)) * on (instance) group_right (domainname) node_uname_info) by (instance, env, nodename) > 80
        for: 5m
        labels:
          severity: high
          owner : profiteroles
          project: evo
          #env: prod
        annotations:
          summary: 'CPU utilization is {{ $value | printf "%.2f" }} % for more than 5m. Threshold 80% CPU util.'
          advice: 'Check CPU utilization'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: EvoHostHighCPULoad65
        expr: sum((100 -(avg by(instance) (rate(node_cpu_seconds_total{mode="idle",project="evo",job="node_exporter",instance!="ja_android",env=~"test|devtest|dev|cert"}[5m])) * 100)) * on (instance) group_right (domainname) node_uname_info) by (instance, env, nodename) > 65
        for: 5m
        labels:
          severity: high
          owner : profiteroles
          project: evo
          #env: prod
        annotations:
          summary: 'CPU utilization is {{ $value | printf "%.2f" }} % for more than 5m. Threshold 65% CPU util.'
          advice: 'Check CPU utilization'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: EvoHostHighCpuIOWait
        expr: sum((avg by(instance) (rate(node_cpu_seconds_total{mode="iowait",project="evo",job="node_exporter",env=~"test|devtest|dev|cert"}[6m])) * 100) * on (instance) group_right (domainname) node_uname_info) by (instance, env, nodename)  > 30
        for: 5m
        labels:
          severity: high
          owner : profiteroles
          project: evo
          #env: prod
        annotations:
          summary: 'CPU IOWAIT is {{ $value | printf "%.2f" }} % for more than 6m. Threshold 30% IOwaits'
          advice: 'Check disks utilization'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: EvoHostOutOfMemory
        expr: node_memory_MemAvailable_bytes {job="node_exporter",project="evo",instance!="ja_android",env=~"test|devtest|dev|cert"} / node_memory_MemTotal_bytes * 100 < 10
        for: 2m
        labels:
          severity: high
          owner : profiteroles
          project: evo
          #env: prod
        annotations:
          summary: 'Host out of memory. Node memory is filling up ({{ $value | printf "%.2f"}}% left). Threshold 10% free memory.'
          advice: 'Check MEMORY utilization'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: HostMemoryUnderMemoryPressure
        expr: rate(node_vmstat_pgmajfault{job="node_exporter",project="evo",env=~"test|devtest|dev|cert"}[1m]) > 1000
        for: 2m
        labels:
          severity: warning
          owner : profiteroles
          project: evo
          #env: prod
        annotations:
          summary: 'The node is under heavy memory pressure. High rate of major page faults {{ $value | printf "%.2f"}}. Threshold 1000 pagefaults'
          advice: 'Check MEMORY utilization and find out the page faults causing app. Threshold 1000 pagefaults'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: HostOutOfInodes
        expr: node_filesystem_files_free{job="node_exporter",project="evo",env=~"test|devtest|dev|cert"} / node_filesystem_files * 100 < 10
        for: 1m
        labels:
          severity: warning
          owner : profiteroles
          project: evo
          #env: prod
        annotations:
          summary: 'Host out of inodes {{ $value | printf "%.2f"}} % left. Threshold 10%.'
          help: 'Check df -hi on host'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: DiskFreeLessThan5%
        expr: ( 100 - ( node_filesystem_avail_bytes { mountpoint !~ "/boot|/home", project = "evo", env=~"test|devtest|dev|cert" } / node_filesystem_size_bytes { mountpoint !~ "/boot|/home", project = "evo", env=~"test|devtest|dev|cert" } * 100 ) ) > 95
        for: 1m
        labels:
          severity: high
          owner : profiteroles
          project: evo
        annotations:
          summary: 'There is {{ $value | printf "%.2f"}}% of disk space used!!! Threshold 95%'
          advice: 'Cleanup disk ASAP!!! / configure data rotation.'
          help: 'df -h'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: EvoTestPodsRestartingTooFrequently
        expr: kube_pod_container_status_restarts_total { project = "evo", env =~ 'test|devtest|dev', origin_prometheus = "Prometheus" } > 100
        for: 1h
        labels:
          severity: normal
          owner : profiteroles
          project: evo
        annotations:
          summary: 'Pods in test are restarting more then 100 times {{ $value }}. See whats happening in OpenShift TEST cluster!'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: Hikari_connections_slow
        expr: sum (hikaricp_connections_acquire_seconds_max {project = "evo", env="prod"}) by (dc, env, namespace, owner, project) > 50
        for: 0m
        labels:
            severity: warning
            env: 'prod'
            project: 'evo'
        annotations:
          description: "Check database, hikaricp connetctions is slow."
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: Alert_pod_CPU
        expr: sum (rate (container_cpu_usage_seconds_total{project = "evo", env = "cert", image!=""}[1m])) by (deployconfig,namespace,env,pod,owner,project) > 5
        for: 30s
        labels:
          severity: normal
          owner: profiteroles
          env: 'cert'
        annotations:
          summary: 'Pod is consuming high CPU {{ $value | printf "%.2f"}} cores avg in 1 m'
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}
      
      - alert: UnusualDiskReadLatency
        expr: rate(node_disk_read_time_seconds_total{project = "evo"}[1m]) / rate(node_disk_reads_completed_total{project = "evo"}[1m]) > 1
        for: 5m
        labels:
            severity: warning
            owner: profiteroles
            #env: 'test'
        annotations:
          summary: "High disk read latency (instance {{ $labels.instance }}), check data storage!"
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: UnusualDiskWriteLatency
        expr: rate(node_disk_write_time_seconds_total{project = "evo"}[1m]) / rate(node_disk_writes_completed_total{project = "evo"}[1m]) > 2
        for: 5m
        labels:
            severity: warning
            owner: profiteroles
            #env: 'test'
        annotations:
          summary: "High disk write latency (instance {{ $labels.instance }}), check data storage!"
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

###PostgresExporter_rules###
      - alert: DatabaseActiveConnections
        expr: (sum (pg_stat_database_numbackends{project="evo",env=~"test|devtest|dev|cert"}) by (instance))/(sum (pg_settings_max_connections{project="evo",env=~"test|devtest|dev|cert"}) by (instance))*100 > 55
        for: 1m
        labels:
          severity: warning
          owner : profiteroles
          #owner: 'kopylov'
          project: evo
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: pg_publication_count #ACHTUNG!!! custom class, custom metric
        expr: pg_publication_count{job="postgresql_exporter", project="evo", env=~"test|devtest|dev|cert"} == 0
        for: 5m
        labels:
          severity: warning
          #owner: 'kopylov'
          owner : profiteroles
          project: evo
          #env: prod
        annotations:
          summary: "Replication of one of the pg_publication_tables table has failed"
          help: "select * from pg_catalog.pg_publication left join pg_catalog.pg_replication_slots on pg_replication_slots.slot_name=pg_publication.pubname"
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

###TEST_rules###
      - alert: TEST_hikari _connections_timeout
        expr: delta(hikaricp_connetctions_timeout_total{env="prod"} [1m]) > 0
        for: 0m
        labels:
            severity: warning
            owner: 'kopylov'
            env: 'test'
        annotations:
          description: "hikaricp_connetctions_timeout_total is growing. Check pod to database connection"
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

      - alert: TEST_hikari_connections
        expr: hikaricp_connetctions {env="prod"} == 0
        for: 0m
        labels:
            severity: warning
            owner: 'kopylov'
            env: 'test'
        annotations:
          description: "hikaricp_connetctions = 0. Check pod to database connection"
          timestamp: >
            {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

     # - alert: TEST_labels
     #   expr: kafka_consumer_last_poll_seconds_ago{project="evo"} > 0
     #   for: 0m
     #   labels:
     #       severity: warning
     #       owner: 'kopylov'
     #       env: 'test'
     #   annotations:
     #     description: '1{{ $labels.owner }}2{ $labels.scope }}3{{ $labels.instance }}4{{ $labels.job }}5{{ $labels.severity }}'
     #     timestamp: >
     #       {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}

     # - alert: TEST_Consumer_last_poll
     #   expr: kafka_consumer_last_poll_seconds_ago{project="evo"} > 100
     #   for: 0m
     #   labels:
     #       severity: warning
     #       owner: 'kopylov'
     #       env: 'test'
     #   annotations:
     #     description: 'Check kafka consumer on pod, last poll was {{ $value | printf "%.2f"}} seconds ago {{ $labels }}'
     #     timestamp: >
     #       {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}
     #
     # - alert: TEST_PostgresqlDeadLocks
     #   expr: increase(pg_stat_database_deadlocks{datname!~"template.*|postgres",project="evo"}[1m]) > 10
     #   for: 0m
     #   labels:
     #       severity: warning
     #       owner: 'kopylov'
     #       env: 'test'
     #   annotations:
     #     description: 'Postgresql having {{ $value | printf "%.2f"}} deadlocks. Threshold 10'
     #
     # - alert: TEST_PostgresqlSlowQueries
     #   expr: pg_slow_queries > 10
     #   for: 2m
     #   labels:
     #       severity: warning
     #       owner: 'kopylov'
     #       env: 'test'
     #   annotations:
     #     description: "PostgreSQL executes slow queries"
     #
     # - alert: TEST_PostgresqlHighRollbackRate
     #   expr: rate(pg_stat_database_xact_rollback{datname!~"template.*",project="evo"}[1m]) / rate(pg_stat_database_xact_commit{datname!~"template.*",project="evo"}[1m]) * 10 > 10
     #   for: 0m
     #   labels:
     #       severity: warning
     #       owner: 'kopylov'
     #       env: 'test'
     #   annotations:
     #     description: 'Ratio of transactions being aborted compared to committed is {{ $value | printf "%.2f"}}%. Threshold 10%' 
     #     timestamp: >
     #       {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}
     #
      #- alert: TEST_Patroni_cluster_status
      #  expr: json_exporter_database_status_scoperole{role="master", env="prod", project="evo"} == 1
      #  for: 0m
      #  labels:
      #    severity: warning
      #    owner: 'kopylov'
      #    env: 'test'
      #  annotations:
      #    summary: 'Database cluster {{ $labels.scope }} changed master to {{ $labels.instance }} https://kb.bank.rrr.vip-clients/pages/viewpage.action?pageId=451335341'
      #    timestamp: >
      #      {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}
      #
      #- alert: EvoBlackboxProbeDown
      #  expr: probe_success { project = "evo" } != 1
      #  for: 30s
      #  labels:
      #    #severity: normal
      #    severity: '{{ $labels.severity }}'
      #    owner: 'kopylov'
      #  annotations:
      #    summary: "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 30s"
      #    timestamp: >
      #      {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}
     # - alert: EvoDbzmPodsRestartingTooFrequently_name
     #   expr: kube_pod_info * on(uid) group_left(instance) (rate ( kube_pod_container_status_restarts_total { project = "evo", namespace =~ 'dbzm.+', env =~ 'cert|prod' } [10m] ) * 600) >= 2
     #   for: 30s
     #   labels:
     #     owner: profiteroles
     #     severity: high
     #   annotations:
     #     summary: 'Pods are restarting too frequently (during past 10 min). alert OpenShift cluster admins and EVO devops ASAP!'
     #     timestamp: >
     #       {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}
     #
      #- alert: EvoAppPodsRestartingTooFrequently_ByDC_test
      #  expr: group((rate ( kube_pod_container_status_restarts_total { project = "evo", env = "prod", namespace != "dbzm-prod" } [10m] ) * 600) * on(uid) group_right(pod) kube_pod_info) by (deployconfig,namespace,node,env,pod,owner,project)>=1
      #  for: 30s
      #  labels:
      #    severity: normal
      #    #severity: '{{ $labels.severity }}'
      #    #owner: '{{ $labels.owner }}'
      #    env: 'test'
      #    owner: 'kopylov'
      #  annotations:
      #    summary: 'Pods are restarting too frequently (during past 10 min). alert OpenShift cluster admins and EVO devops ASAP!'
      #    timestamp: >
      #      {{ with query "time()+10800" }}{{ . | first | value | humanizeTimestamp | printf "%.19s UTC+3"}}{{ end }}



